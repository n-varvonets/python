import webbrowser
print(" -    1. Общие моментьі    - ")
# 1.1. ElasticSearch - принимает на вход документ, обрабатьівает его определнньім образом
# и "производньіе" от входньіх данньіх складьівает в память.
# 1.2. Составляющие анализатора(последовательно):
#       - 1 charter filters: убират или изеняет символьі(например - спиливает все html-теги). Может бьіть СКОЛЬКО УГОДНО фильтров в составе анализатора;
#       - 2 tokenizer: разбивает текст на блоки текста, т.е. на некие "токеньі". Он может бьіть ТОЛЬКО ОДИН в составе анализатора;
#       - 3 token filters: принимает на вход то, что получиось после токенизации. Модифицирует полученььіе токеньі(lowercase).
#           Он может как отсутовать, так иметь до нескольких филтрова в себе.
# 1.3. В еластике есть куча стандартньіх токенайзеров, фильтров символов и фильтров токенов.
print(" -    2. Работа стандартного анализатора    - ")
webbrowser.open("https://ibb.co/M55xFVy")
print(" -    3. Работа анализатора в Kibana   - ")
# по дефолту применится стандартный анализатор
POST /_analyze
{"text": "Мама мыла раму"}

# разрубает на токены по пробелам
POST /_analyze
{"text": "Мама мыла раму",
"analyzer": "whitespace"}

# добавляем фильтрацию - нужно ОБЯЗАТЕЛЬНО убрать поле анализатора, ведь мы теперь пишем свой!
POST /_analyze
{"text": "С днём рождения!",
"char_filter": [
  {
      "type": "mapping",
      "mappings": [
        "с => ''",
        "к => ''",
        "С => ''",
        "К => ''"
      ]
    }
  ],
"tokenizer": "standard",
"filter": ["uppercase"]
}

# ещё один пример
GET /_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "٠ => 0",
        "١ => 1",
        "٢ => 2",
        "٣ => 3",
        "٤ => 4",
        "٥ => 5",
        "٦ => 6",
        "٧ => 7",
        "٨ => 8",
        "٩ => 9"
      ]
    }
  ],
  "text": "My license plate is ٢٥٠١٥"
}


