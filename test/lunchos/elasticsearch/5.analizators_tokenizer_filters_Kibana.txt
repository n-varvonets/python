# по дефолту применится стандартный анализатор
POST /_analyze
{"text": "Мама мыла раму"}

# разрубает на токены по пробелам
POST /_analyze
{"text": "Мама мыла раму",
"analyzer": "whitespace"}

# добавляем фильтрацию - нужно ОБЯЗАТЕЛЬНО убрать поле анализатора, ведь мы теперь пишем свой!
POST /_analyze
{"text": "С днём рождения!",
"char_filter": [
  {
      "type": "mapping",
      "mappings": [
        "с => ''",
        "к => ''",
        "С => ''",
        "К => ''"
      ]
    }
  ],
"tokenizer": "standard",
"filter": ["uppercase"]
}

# ещё один пример
GET /_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "٠ => 0",
        "١ => 1",
        "٢ => 2",
        "٣ => 3",
        "٤ => 4",
        "٥ => 5",
        "٦ => 6",
        "٧ => 7",
        "٨ => 8",
        "٩ => 9"
      ]
    }
  ],
  "text": "My license plate is ٢٥٠١٥"
}


